{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765d4fe1-60f6-4e05-943f-cd8ff60f1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef73af0-f0e6-4ce6-86cc-f7a40158ee7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-P5ATL43H:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Streaming with File</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2a007389270>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession  \\\n",
    "        .builder  \\\n",
    "        .appName('Spark Streaming with File')  \\\n",
    "        .config('spark.streaming.stopGracefullyOnShutdown', True)  \\\n",
    "        .master(\"local[*]\")  \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f7b19f-0757-410a-8b7a-a4a0864593d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customerId: string, data: struct<devices:array<struct<deviceId:string,measure:string,status:string,temperature:bigint>>>, eventId: string, eventOffset: bigint, eventPublisher: string, eventTime: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.streaming.schemaInference', True)\n",
    "\n",
    "# Creating the streaming_df to read from input directory\n",
    "streaming_df = spark.readStream  \\\n",
    "                    .format(\"json\")  \\\n",
    "                    .option(\"cleanSource\", \"archive\")  \\\n",
    "                    .option(\"sourceArchiveDir\", \"archives\\\\device_data\\\\\")  \\\n",
    "                    .option(\"maxFilesPerTrigger\", 1)  \\\n",
    "                    .load(\"data\\\\input\\\\\")\n",
    "\n",
    "streaming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7d3feb1-c207-4640-a799-ec390424fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4b6be9-c449-49b0-8065-0aa466c05aa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();;\nFileSource[data\\input\\]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstreaming_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\big_data\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py:442\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical))\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Documents\\big_data\\spark-3.0.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\Documents\\big_data\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:137\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    133\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();;\nFileSource[data\\input\\]"
     ]
    }
   ],
   "source": [
    "streaming_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feff8468-f31c-4d95-913c-e0da64d8b0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+\n",
      "|customerId|data                                                                   |eventId                             |eventOffset|eventPublisher|eventTime                 |\n",
      "+----------+-----------------------------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+\n",
      "|CI00119   |[[]]                                                                   |ba2ea9f4-a5d9-434e-8e4d-1c80c2d4b456|10000      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00103   |[[[D001, C, ERROR, 15], [D002, C, SUCCESS, 16]]]                       |e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00104   |[[]]                                                                   |8c202190-bc24-4485-89ec-de78e602dd68|10002      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00108   |[[[D004, C, SUCCESS, 16]]]                                             |aa90011f-3967-496c-b94b-a0c8de19a3d3|10003      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00116   |[[]]                                                                   |e8859641-e9ad-44f8-94ce-353b840cff73|10004      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00106   |[[[D002, C, ERROR, 30], [D001, C, STANDBY, 10], [D001, C, SUCCESS, 6]]]|804e8fa3-307b-482e-b629-af880c52e884|10005      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00120   |[[]]                                                                   |b8675032-3fdf-4e1e-8816-3d4c1cd852cf|10006      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00119   |[[[D002, C, ERROR, 15], [D002, C, SUCCESS, 12]]]                       |1c8d9682-56f0-4c3d-95c8-fce1bac45a74|10007      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00117   |[[[D003, C, ERROR, 6], [D001, C, ERROR, 19], [D005, C, ERROR, 0]]]     |fc625d0e-06c2-46b1-b9b5-b4a067e0a212|10008      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00100   |[[[D003, C, STANDBY, 27], [D001, C, SUCCESS, 24]]]                     |7dba5625-33e9-4d9f-b767-b44bd03e098d|10009      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00118   |[[[D002, C, SUCCESS, 27], [D005, C, STANDBY, 23]]]                     |209cab2d-7934-4ad2-ac36-dcae0b42d96b|10010      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00119   |[[]]                                                                   |158c282f-3bbc-447a-9303-1e978a23274a|10011      |device        |2023-01-05 11:13:53.643364|\n",
      "|CI00117   |[[[D002, C, SUCCESS, 5]]]                                              |7146c4a8-54ed-4075-b013-c2d99e65d295|10012      |device        |2023-01-05 11:13:53.643895|\n",
      "|CI00103   |[[[D004, C, SUCCESS, 23]]]                                             |1ff547fd-e335-457e-9a1f-686cfbe903e3|10013      |device        |2023-01-05 11:13:53.643895|\n",
      "|CI00109   |[[[D003, C, ERROR, 18]]]                                               |692e9999-1110-4441-a20e-fd76692e2c17|10014      |device        |2023-01-05 11:13:53.643895|\n",
      "|CI00101   |[[]]                                                                   |80101e8c-af6a-4ff5-81ae-3bf5db432811|10015      |device        |2023-01-05 11:13:53.649684|\n",
      "|CI00102   |[[]]                                                                   |7f0b1fba-3cd1-440d-9203-5dea57057ca8|10016      |device        |2023-01-05 11:13:53.649684|\n",
      "|CI00104   |[[[D004, C, STANDBY, 5], [D004, C, SUCCESS, 22], [D004, C, ERROR, 9]]] |cb8a6a8f-89c9-498a-9106-7d148ba998b7|10017      |device        |2023-01-05 11:13:53.649684|\n",
      "|CI00118   |[[]]                                                                   |a920562e-e8c0-4884-ad28-b74d82fc9ad8|10018      |device        |2023-01-05 11:13:53.649684|\n",
      "|CI00104   |[[[D005, C, ERROR, 20], [D005, C, STANDBY, 4]]]                        |87941320-3424-42dc-b853-371698b9e7dd|10019      |device        |2023-01-05 11:13:53.649684|\n",
      "+----------+-----------------------------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to display the output with show, we should use read instead of readStream\n",
    "\n",
    "non_streaming_df = spark.read  \\\n",
    "                    .format(\"json\")  \\\n",
    "                    .option(\"cleanSource\", \"archive\")  \\\n",
    "                    .option(\"sourceArchiveDir\", \"archives\\\\device_data\\\\\")  \\\n",
    "                    .option(\"maxFilesPerTrigger\", 1)  \\\n",
    "                    .load(\"data\\\\input\\\\\")\n",
    "\n",
    "non_streaming_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9792201f-49a3-45ee-ba6c-721533a16525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "non_streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "375250cd-4045-4f69-bf20-195c5afed946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------+-----------+--------------+--------------------------+----------------------+\n",
      "|customerId|eventId                             |eventOffset|eventPublisher|eventTime                 |devices               |\n",
      "+----------+------------------------------------+-----------+--------------+--------------------------+----------------------+\n",
      "|CI00103   |e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |2023-01-05 11:13:53.643364|[D001, C, ERROR, 15]  |\n",
      "|CI00103   |e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |2023-01-05 11:13:53.643364|[D002, C, SUCCESS, 16]|\n",
      "|CI00108   |aa90011f-3967-496c-b94b-a0c8de19a3d3|10003      |device        |2023-01-05 11:13:53.643364|[D004, C, SUCCESS, 16]|\n",
      "|CI00106   |804e8fa3-307b-482e-b629-af880c52e884|10005      |device        |2023-01-05 11:13:53.643364|[D002, C, ERROR, 30]  |\n",
      "|CI00106   |804e8fa3-307b-482e-b629-af880c52e884|10005      |device        |2023-01-05 11:13:53.643364|[D001, C, STANDBY, 10]|\n",
      "|CI00106   |804e8fa3-307b-482e-b629-af880c52e884|10005      |device        |2023-01-05 11:13:53.643364|[D001, C, SUCCESS, 6] |\n",
      "|CI00119   |1c8d9682-56f0-4c3d-95c8-fce1bac45a74|10007      |device        |2023-01-05 11:13:53.643364|[D002, C, ERROR, 15]  |\n",
      "|CI00119   |1c8d9682-56f0-4c3d-95c8-fce1bac45a74|10007      |device        |2023-01-05 11:13:53.643364|[D002, C, SUCCESS, 12]|\n",
      "|CI00117   |fc625d0e-06c2-46b1-b9b5-b4a067e0a212|10008      |device        |2023-01-05 11:13:53.643364|[D003, C, ERROR, 6]   |\n",
      "|CI00117   |fc625d0e-06c2-46b1-b9b5-b4a067e0a212|10008      |device        |2023-01-05 11:13:53.643364|[D001, C, ERROR, 19]  |\n",
      "|CI00117   |fc625d0e-06c2-46b1-b9b5-b4a067e0a212|10008      |device        |2023-01-05 11:13:53.643364|[D005, C, ERROR, 0]   |\n",
      "|CI00100   |7dba5625-33e9-4d9f-b767-b44bd03e098d|10009      |device        |2023-01-05 11:13:53.643364|[D003, C, STANDBY, 27]|\n",
      "|CI00100   |7dba5625-33e9-4d9f-b767-b44bd03e098d|10009      |device        |2023-01-05 11:13:53.643364|[D001, C, SUCCESS, 24]|\n",
      "|CI00118   |209cab2d-7934-4ad2-ac36-dcae0b42d96b|10010      |device        |2023-01-05 11:13:53.643364|[D002, C, SUCCESS, 27]|\n",
      "|CI00118   |209cab2d-7934-4ad2-ac36-dcae0b42d96b|10010      |device        |2023-01-05 11:13:53.643364|[D005, C, STANDBY, 23]|\n",
      "|CI00117   |7146c4a8-54ed-4075-b013-c2d99e65d295|10012      |device        |2023-01-05 11:13:53.643895|[D002, C, SUCCESS, 5] |\n",
      "|CI00103   |1ff547fd-e335-457e-9a1f-686cfbe903e3|10013      |device        |2023-01-05 11:13:53.643895|[D004, C, SUCCESS, 23]|\n",
      "|CI00109   |692e9999-1110-4441-a20e-fd76692e2c17|10014      |device        |2023-01-05 11:13:53.643895|[D003, C, ERROR, 18]  |\n",
      "|CI00104   |cb8a6a8f-89c9-498a-9106-7d148ba998b7|10017      |device        |2023-01-05 11:13:53.649684|[D004, C, STANDBY, 5] |\n",
      "|CI00104   |cb8a6a8f-89c9-498a-9106-7d148ba998b7|10017      |device        |2023-01-05 11:13:53.649684|[D004, C, SUCCESS, 22]|\n",
      "+----------+------------------------------------+-----------+--------------+--------------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "exploded_df = non_streaming_df.select(\"customerId\", \"eventId\", \"eventOffset\", \"eventPublisher\", \"eventTime\", \"data\")  \\\n",
    "                                .withColumn(\"devices\", explode('data.devices'))  \\\n",
    "                                .drop(\"data\")\n",
    "\n",
    "exploded_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e235bd2-d919-43e5-a795-f1e1559d197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- devices: struct (nullable = true)\n",
      " |    |-- deviceId: string (nullable = true)\n",
      " |    |-- measure: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afb37161-5f1d-4a62-bc5f-3b95e264eac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------+-----------+--------------+--------------------------+--------+-------+-------+-----------+\n",
      "|customerId|eventId                             |eventOffset|eventPublisher|eventTime                 |deviceId|measure|status |temperature|\n",
      "+----------+------------------------------------+-----------+--------------+--------------------------+--------+-------+-------+-----------+\n",
      "|CI00103   |e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |2023-01-05 11:13:53.643364|D001    |C      |ERROR  |15         |\n",
      "|CI00103   |e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |2023-01-05 11:13:53.643364|D002    |C      |SUCCESS|16         |\n",
      "|CI00108   |aa90011f-3967-496c-b94b-a0c8de19a3d3|10003      |device        |2023-01-05 11:13:53.643364|D004    |C      |SUCCESS|16         |\n",
      "|CI00106   |804e8fa3-307b-482e-b629-af880c52e884|10005      |device        |2023-01-05 11:13:53.643364|D002    |C      |ERROR  |30         |\n",
      "|CI00106   |804e8fa3-307b-482e-b629-af880c52e884|10005      |device        |2023-01-05 11:13:53.643364|D001    |C      |STANDBY|10         |\n",
      "|CI00106   |804e8fa3-307b-482e-b629-af880c52e884|10005      |device        |2023-01-05 11:13:53.643364|D001    |C      |SUCCESS|6          |\n",
      "|CI00119   |1c8d9682-56f0-4c3d-95c8-fce1bac45a74|10007      |device        |2023-01-05 11:13:53.643364|D002    |C      |ERROR  |15         |\n",
      "|CI00119   |1c8d9682-56f0-4c3d-95c8-fce1bac45a74|10007      |device        |2023-01-05 11:13:53.643364|D002    |C      |SUCCESS|12         |\n",
      "|CI00117   |fc625d0e-06c2-46b1-b9b5-b4a067e0a212|10008      |device        |2023-01-05 11:13:53.643364|D003    |C      |ERROR  |6          |\n",
      "|CI00117   |fc625d0e-06c2-46b1-b9b5-b4a067e0a212|10008      |device        |2023-01-05 11:13:53.643364|D001    |C      |ERROR  |19         |\n",
      "|CI00117   |fc625d0e-06c2-46b1-b9b5-b4a067e0a212|10008      |device        |2023-01-05 11:13:53.643364|D005    |C      |ERROR  |0          |\n",
      "|CI00100   |7dba5625-33e9-4d9f-b767-b44bd03e098d|10009      |device        |2023-01-05 11:13:53.643364|D003    |C      |STANDBY|27         |\n",
      "|CI00100   |7dba5625-33e9-4d9f-b767-b44bd03e098d|10009      |device        |2023-01-05 11:13:53.643364|D001    |C      |SUCCESS|24         |\n",
      "|CI00118   |209cab2d-7934-4ad2-ac36-dcae0b42d96b|10010      |device        |2023-01-05 11:13:53.643364|D002    |C      |SUCCESS|27         |\n",
      "|CI00118   |209cab2d-7934-4ad2-ac36-dcae0b42d96b|10010      |device        |2023-01-05 11:13:53.643364|D005    |C      |STANDBY|23         |\n",
      "|CI00117   |7146c4a8-54ed-4075-b013-c2d99e65d295|10012      |device        |2023-01-05 11:13:53.643895|D002    |C      |SUCCESS|5          |\n",
      "|CI00103   |1ff547fd-e335-457e-9a1f-686cfbe903e3|10013      |device        |2023-01-05 11:13:53.643895|D004    |C      |SUCCESS|23         |\n",
      "|CI00109   |692e9999-1110-4441-a20e-fd76692e2c17|10014      |device        |2023-01-05 11:13:53.643895|D003    |C      |ERROR  |18         |\n",
      "|CI00104   |cb8a6a8f-89c9-498a-9106-7d148ba998b7|10017      |device        |2023-01-05 11:13:53.649684|D004    |C      |STANDBY|5          |\n",
      "|CI00104   |cb8a6a8f-89c9-498a-9106-7d148ba998b7|10017      |device        |2023-01-05 11:13:53.649684|D004    |C      |SUCCESS|22         |\n",
      "+----------+------------------------------------+-----------+--------------+--------------------------+--------+-------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattened_df = exploded_df.selectExpr(\"customerId\", \"eventId\", \"eventOffset\", \"eventPublisher\", \"eventTime\", \n",
    "                                     \"devices.deviceId as deviceId\", \"devices.measure as measure\",\n",
    "                                      \"devices.status as status\", \"devices.temperature as temperature\"\n",
    "                                     )\n",
    "flattened_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4628dae2-d982-410d-a0dd-d57d6b8e1387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735a430-f04b-4901-99d9-791a1e42ab7a",
   "metadata": {},
   "source": [
    "Streaming source and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee7dc9d9-4be8-45d8-867d-13c6da04af1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customerId: string, data: struct<devices:array<struct<deviceId:string,measure:string,status:string,temperature:bigint>>>, eventId: string, eventOffset: bigint, eventPublisher: string, eventTime: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.streaming.schemaInference', True)\n",
    "\n",
    "# Creating the streaming_df to read from input directory\n",
    "streaming_df = spark.readStream  \\\n",
    "                    .format(\"json\")  \\\n",
    "                    .option(\"cleanSource\", \"archive\")  \\\n",
    "                    .option(\"sourceArchiveDir\", \"archives\\\\device_data\\\\\")  \\\n",
    "                    .option(\"maxFilesPerTrigger\", 1)  \\\n",
    "                    .load(\"data\\\\input\\\\\")\n",
    "\n",
    "streaming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3592b302-5774-485f-b04a-24a91544fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "exploded_df = streaming_df.select(\"customerId\", \"eventId\", \"eventOffset\", \"eventPublisher\", \"eventTime\", \"data\")  \\\n",
    "                                .withColumn(\"devices\", explode('data.devices'))  \\\n",
    "                                .drop(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53850cdf-edae-4ab8-81cb-a70539435acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_df = exploded_df.selectExpr(\"customerId\", \"eventId\", \"eventOffset\", \"eventPublisher\", \"eventTime\", \n",
    "                                     \"devices.deviceId as deviceId\", \"devices.measure as measure\",\n",
    "                                      \"devices.status as status\", \"devices.temperature as temperature\"\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242042a-d978-4240-af0a-e3fbe1efbc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to console sink to check the output\n",
    "\n",
    "writing_df = flattened_df.writeStream  \\\n",
    "                        .format('json')  \\\n",
    "                        .option('path', 'output\\\\device_data\\\\')  \\\n",
    "                        .option(\"checkpointLocation\", \"checkpoint_dir\")  \\\n",
    "                        .outputMode(\"append\")  \\\n",
    "                        .start()\n",
    "\n",
    "# Start the streaming application to run until the following happens\n",
    "# 1. Exception in the running program\n",
    "# 2. Manual Interruption\n",
    "writing_df.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8be0f2c-7af4-4349-8ccc-80e2f1b8410a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Unable to infer schema for JSON at . It must be specified manually;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check the data at the output location\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m out_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdevice_data\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m out_df\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Documents\\big_data\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py:300\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[1;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[0;32m    298\u001b[0m     path \u001b[38;5;241m=\u001b[39m [path]\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32m~\\Documents\\big_data\\spark-3.0.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\Documents\\big_data\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:137\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    133\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Unable to infer schema for JSON at . It must be specified manually;"
     ]
    }
   ],
   "source": [
    "# Check the data at the output location\n",
    "\n",
    "out_df = spark.read.json(\"output\\\\device_data\\\\\")\n",
    "out_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f17064a-20e1-493d-aedd-ed5574f98eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courtesy : https://github.com/subhamkharwal/ease-with-apache-spark/blob/master/32_spark_streaming_read_from_files.ipynb\n",
    "\n",
    "# https://urlit.me/blog/pyspark-structured-streaming-read-from-files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758a623-bfbf-4f08-9879-ac6fb7ad9aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
