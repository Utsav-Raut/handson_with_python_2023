{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12516cf1-728f-4a56-9acb-c488966bda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad825bce-9edc-43dc-b6ce-7a5758e55feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession  \\\n",
    "        .builder  \\\n",
    "        .appName('Transformations1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e364ea3b-ff3d-4e91-aacb-37b5098e92f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 7),\n",
       " ('A', 8),\n",
       " ('A', -4),\n",
       " ('B', 3),\n",
       " ('B', 9),\n",
       " ('B', -1),\n",
       " ('C', 1),\n",
       " ('C', 5),\n",
       " ('C', 9)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples = [('A', 7), ('A', 8), ('A', -4),\n",
    "         ('B', 3), ('B', 9), ('B', -1),\n",
    "         ('C', 1), ('C', 5), ('C', 9)]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(tuples)\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dbba41c-925e-477c-b266-a7454ed6c15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C', 9), ('C', 5), ('C', 1), ('B', 9), ('B', 3), ('A', 8), ('A', 7)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the negative values\n",
    "\n",
    "positives = rdd1.filter(lambda x: x[1] > 0)\n",
    "positives.top(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9c5a06d-4e13-4ae3-a3a3-5a3aa371225a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', <pyspark.resultiterable.ResultIterable at 0x258c0c53370>),\n",
       " ('C', <pyspark.resultiterable.ResultIterable at 0x258c0c53970>),\n",
       " ('A', <pyspark.resultiterable.ResultIterable at 0x258c0c532b0>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7652b401-d7b6-4b26-9ee5-f670e825b8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (12, 6.0)), ('C', (15, 5.0)), ('A', (15, 7.5))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find sum and average per key using groupByKey()\n",
    "\n",
    "sum_and_avg = positives.groupByKey()  \\\n",
    "                .mapValues(lambda v: (sum(v), float(sum(v))/len(v)))\n",
    "\n",
    "sum_and_avg.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7924a08-41bc-441b-8397-127008cc8048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (7, 1)),\n",
       " ('A', (8, 1)),\n",
       " ('B', (3, 1)),\n",
       " ('B', (9, 1)),\n",
       " ('C', (1, 1)),\n",
       " ('C', (5, 1)),\n",
       " ('C', (9, 1))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find sum and average using reduceByKey()\n",
    "\n",
    "sum_count = positives.mapValues(lambda v: (v, 1))\n",
    "sum_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa83d8db-ae20-4961-b72c-8f9e83a31d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (12, 2)), ('C', (15, 3)), ('A', (15, 2))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_count_agg2 = sum_count.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "sum_count_agg2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d987f9c-92e5-400f-b68d-699a2c8f4ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (12, 6.0)), ('C', (15, 5.0)), ('A', (15, 7.5))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_and_avg2 = sum_count_agg2.mapValues(lambda x: (x[0], float(x[0] / x[1])))\n",
    "sum_and_avg2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf338095-1c3b-44e9-9f4d-ea8a4836addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The groupByKey() transformation groups the values for each key\n",
    "# in the RDD into a single sequence, similar to a SQL GROUP BY statement.\n",
    "# This transformation can cause out of memory (OOM) errors\n",
    "# as data is sent over the network of Spark servers and collected on\n",
    "# the reducer/workers when the number of values per key is in the\n",
    "# thousands or millions.\n",
    "# With the reduceByKey() transformation, however, data is combined\n",
    "# in each partition, so there is only one output for each key in\n",
    "# each partition to send over the network of Spark servers. This\n",
    "# makes it more scalable than groupByKey(). reduceByKey() merges\n",
    "# the values for each key using an associative and commutative\n",
    "# reduce function. It combines all the values (per key) into another\n",
    "# value with the exact same data type (this is a limitation, which can\n",
    "# be overcome by using the combineByKey() transformation). Overall,\n",
    "# the reduceByKey() is more scaleable than the groupByKey()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86f31a-b15b-47ed-8bd2-493789c331fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
