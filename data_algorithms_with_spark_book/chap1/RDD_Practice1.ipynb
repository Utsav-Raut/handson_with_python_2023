{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2734fc52-3d23-4d4e-8330-aaa9a10dea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3cb599-4991-4975-84e1-b228373c3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of SparkSession\n",
    "\n",
    "spark = SparkSession  \\\n",
    "        .builder  \\\n",
    "        .appName('Sample Demo')  \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4294a71-8aee-4a0a-b1b2-5306b2ed5543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Red,Fox,is,fast']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD[String], which represents all input \n",
    "# records; each record becomes an RDD element\n",
    "\n",
    "records = spark.sparkContext.textFile(\"data\\\\sample1.txt\")\n",
    "records.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778c94a7-f427-4c06-802d-fe40bddf6556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red,fox,is,fast']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each element of the RDD to lowercase\n",
    "# x denotes a single element of the RDD\n",
    "# records: source RDD[String]\n",
    "# records_lowercase :  target RDD[String]\n",
    "\n",
    "records_lowercase = records.map(lambda x : x.lower())\n",
    "records_lowercase.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f481e3d9-86ef-4079-82e1-c65ef4ddd86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'fox', 'is', 'fast']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split each record into a list of words\n",
    "# records_lowercase : source RDD[String]\n",
    "# words : target RDD[String]\n",
    "\n",
    "words = records_lowercase.flatMap(lambda x: x.split(','))\n",
    "words.collect()\n",
    "# Here all the elements of the source RDD is first splitted by commas,\n",
    "# then flattened using the flatMap transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6cf10d5-d45d-4325-9635-56db3938fff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'fox', 'fast']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep words with a length greater than 2\n",
    "# x denotes a word\n",
    "# words : source RDD[String]\n",
    "# filtered : target RDD[String]\n",
    "\n",
    "filtered = words.filter(lambda x: len(x) > 2)\n",
    "filtered.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6e1527-9c2e-466a-a5c4-99b758792950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can observe, Spark transformations are high-level, powerful, and simple.\n",
    "# Spark is by nature distributed and parallel: your input data is partitioned and can be\n",
    "# processed by transformations (such as mappers, filters, and reducers) in parallel in a\n",
    "# cluster environment. In a nutshell, to solve a data analytics problem in PySpark, you\n",
    "# read data and represent it as an RDD or DataFrame (depending on the nature of the\n",
    "# data format), then write a set of transformations to convert your data into the desired\n",
    "# output. Spark automatically partitions your DataFrames and RDDs and distributes\n",
    "# the partitions across different cluster nodes. Partitions are the basic units of parallelism\n",
    "# in Spark. Parallelism is what allows developers to perform tasks on hundreds of\n",
    "# computer servers in a cluster in parallel and independently. A partition in Spark is a\n",
    "# chunk (a logical division) of data stored on a node in the cluster. DataFrames and\n",
    "# RDDs are collections of partitions. Spark has a default data partitioner for RDDs and\n",
    "# DataFrames, but you may override that partitioning with your own custom\n",
    "# programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69363386-09d4-4184-83ae-f540bcf6a4ca",
   "metadata": {},
   "source": [
    "# Creating RDDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97e78280-c7de-453c-8875-af12c0d90a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=Sample Demo>\n"
     ]
    }
   ],
   "source": [
    "# Method 1:\n",
    "\n",
    "spark_session = SparkSession.builder.appName('Type1').getOrCreate()\n",
    "spark_context = spark_session.sparkContext\n",
    "print(spark_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5cf62e5-72d3-42a9-9586-8e0a73a02d07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Sample Demo, master=local[*]) created by getOrCreate at C:\\Users\\Boom\\AppData\\Local\\Temp\\ipykernel_7072\\1696457726.py:6 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Method 2:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[1;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmyapp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(sc)\n",
      "File \u001b[1;32m~\\Documents\\big_data\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\context.py:128\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 128\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    131\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32m~\\Documents\\big_data\\spark-3.0.0-bin-hadoop3.2\\python\\pyspark\\context.py:331\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    328\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;241m%\u001b[39m (currentAppName, currentMaster,\n\u001b[0;32m    336\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction, callsite\u001b[38;5;241m.\u001b[39mfile, callsite\u001b[38;5;241m.\u001b[39mlinenum))\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Sample Demo, master=local[*]) created by getOrCreate at C:\\Users\\Boom\\AppData\\Local\\Temp\\ipykernel_7072\\1696457726.py:6 "
     ]
    }
   ],
   "source": [
    "# Method 2:\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext('local', 'myapp')\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0972553e-c095-4a5d-bead-7d976dc63048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
